{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from math import log2\nimport random\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.utils import save_image\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2022-10-08T18:15:52.225271Z","iopub.execute_input":"2022-10-08T18:15:52.225889Z","iopub.status.idle":"2022-10-08T18:15:52.231982Z","shell.execute_reply.started":"2022-10-08T18:15:52.225854Z","shell.execute_reply":"2022-10-08T18:15:52.231031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2022-10-08T18:15:52.474470Z","iopub.execute_input":"2022-10-08T18:15:52.474796Z","iopub.status.idle":"2022-10-08T18:15:52.480722Z","shell.execute_reply.started":"2022-10-08T18:15:52.474764Z","shell.execute_reply":"2022-10-08T18:15:52.479697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATASET                 = '../input/women-clothes/'\nSTART_TRAIN_AT_IMG_SIZE = 4\nDEVICE                  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nLEARNING_RATE           = 1e-3\nBATCH_SIZES             = [32, 32, 32, 16, 16, 16]\nimage_size              = 128\nCHANNELS_IMG            = 3\nZ_DIM                   = 256  \nIN_CHANNELS             = 256  \nCRITIC_ITERATIONS       = 1\nLAMBDA_GP               = 10\nPROGRESSIVE_EPOCHS      = [30] * len(BATCH_SIZES)\nFIXED_NOISE             = torch.randn(9, Z_DIM, 1, 1).to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2022-10-08T18:15:52.770636Z","iopub.execute_input":"2022-10-08T18:15:52.770970Z","iopub.status.idle":"2022-10-08T18:15:52.778409Z","shell.execute_reply.started":"2022-10-08T18:15:52.770943Z","shell.execute_reply":"2022-10-08T18:15:52.777353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_loader(image_size):\n    transform = transforms.Compose(\n        [\n            transforms.Resize((image_size, image_size)),\n            transforms.ToTensor(),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.Normalize(\n                [0.5 for _ in range(CHANNELS_IMG)],\n                [0.5 for _ in range(CHANNELS_IMG)],\n            ),\n        ]\n    )\n    batch_size = BATCH_SIZES[int(log2(image_size / 4))]\n    dataset = datasets.ImageFolder(root=DATASET, transform=transform)\n    loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True,\n    )\n    return loader, dataset\n","metadata":{"execution":{"iopub.status.busy":"2022-10-08T18:15:53.074846Z","iopub.execute_input":"2022-10-08T18:15:53.075485Z","iopub.status.idle":"2022-10-08T18:15:53.083291Z","shell.execute_reply.started":"2022-10-08T18:15:53.075449Z","shell.execute_reply":"2022-10-08T18:15:53.082352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_loader():\n    loader,_ = get_loader(128)\n    cloth ,_ = next(iter(loader))\n    _, ax = plt.subplots(2,3, figsize=(8,5))\n    plt.suptitle('Some real samples', fontsize=15, fontweight='bold')\n    ind = 0 \n    for k in range(2):\n        for kk in range(3):\n            \n            ax[k][kk].imshow((cloth[ind].permute(1,2,0)+1)/2) \n            ind += 1\ncheck_loader()","metadata":{"execution":{"iopub.status.busy":"2022-10-08T18:15:54.363745Z","iopub.execute_input":"2022-10-08T18:15:54.364570Z","iopub.status.idle":"2022-10-08T18:15:57.201197Z","shell.execute_reply.started":"2022-10-08T18:15:54.364529Z","shell.execute_reply":"2022-10-08T18:15:57.200279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32]\n\n\nclass WSConv2d(nn.Module):\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2\n    ):\n        super(WSConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n        self.bias = self.conv.bias\n        self.conv.bias = None\n\n        nn.init.normal_(self.conv.weight)\n        nn.init.zeros_(self.bias)\n\n    def forward(self, x):\n        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n\n\nclass PixelNorm(nn.Module):\n    def __init__(self):\n        super(PixelNorm, self).__init__()\n        self.epsilon = 1e-8\n\n    def forward(self, x):\n        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n        super(ConvBlock, self).__init__()\n        self.use_pn = use_pixelnorm\n        self.conv1 = WSConv2d(in_channels, out_channels)\n        self.conv2 = WSConv2d(out_channels, out_channels)\n        self.leaky = nn.LeakyReLU(0.2)\n        self.pn = PixelNorm()\n\n    def forward(self, x):\n        x = self.leaky(self.conv1(x))\n        x = self.pn(x) if self.use_pn else x\n        x = self.leaky(self.conv2(x))\n        x = self.pn(x) if self.use_pn else x\n        return x\n\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim, in_channels, img_channels=3):\n        super(Generator, self).__init__()\n\n        self.initial = nn.Sequential(\n            PixelNorm(),\n            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),\n            nn.LeakyReLU(0.2),\n            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2),\n            PixelNorm(),\n        )\n\n        self.initial_rgb = WSConv2d(\n            in_channels, img_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.prog_blocks, self.rgb_layers = (\n            nn.ModuleList([]),\n            nn.ModuleList([self.initial_rgb]),\n        )\n\n        for i in range(\n            len(factors) - 1\n        ):  \n            conv_in_c = int(in_channels * factors[i])\n            conv_out_c = int(in_channels * factors[i + 1])\n            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n            self.rgb_layers.append(\n                WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n            )\n\n    def fade_in(self, alpha, upscaled, generated):\n        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n\n    def forward(self, x, alpha, steps):\n        out = self.initial(x)\n\n        if steps == 0:\n            return self.initial_rgb(out)\n\n        for step in range(steps):\n            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n            out = self.prog_blocks[step](upscaled)\n\n        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n        final_out = self.rgb_layers[steps](out)\n        return self.fade_in(alpha, final_upscaled, final_out)\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, z_dim, in_channels, img_channels=3):\n        super(Discriminator, self).__init__()\n        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n        self.leaky = nn.LeakyReLU(0.2)\n\n\n        for i in range(len(factors) - 1, 0, -1):\n            conv_in = int(in_channels * factors[i])\n            conv_out = int(in_channels * factors[i - 1])\n            self.prog_blocks.append(ConvBlock(conv_in, conv_out, use_pixelnorm=False))\n            self.rgb_layers.append(\n                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n            )\n\n        self.initial_rgb = WSConv2d(\n            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.rgb_layers.append(self.initial_rgb)\n        self.avg_pool = nn.AvgPool2d(\n            kernel_size=2, stride=2\n        )  \n        self.final_block = nn.Sequential(\n            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n            nn.LeakyReLU(0.2),\n            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n            nn.LeakyReLU(0.2),\n            WSConv2d(\n                in_channels, 1, kernel_size=1, padding=0, stride=1\n            ), \n        )\n\n    def fade_in(self, alpha, downscaled, out):\n\n        return alpha * out + (1 - alpha) * downscaled\n\n    def minibatch_std(self, x):\n        batch_statistics = (\n            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n        )\n\n        return torch.cat([x, batch_statistics], dim=1)\n\n    def forward(self, x, alpha, steps):\n\n        cur_step = len(self.prog_blocks) - steps\n\n\n        out = self.leaky(self.rgb_layers[cur_step](x))\n\n        if steps == 0:  \n            out = self.minibatch_std(out)\n            return self.final_block(out).view(out.shape[0], -1)\n\n\n        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n        out = self.avg_pool(self.prog_blocks[cur_step](out))\n\n\n        out = self.fade_in(alpha, downscaled, out)\n\n        for step in range(cur_step + 1, len(self.prog_blocks)):\n            out = self.prog_blocks[step](out)\n            out = self.avg_pool(out)\n\n        out = self.minibatch_std(out)\n        return self.final_block(out).view(out.shape[0], -1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n    BATCH_SIZE, C, H, W = real.shape\n    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n    interpolated_images = real * beta + fake.detach() * (1 - beta)\n    interpolated_images.requires_grad_(True)\n\n    # Calculate critic scores\n    mixed_scores = critic(interpolated_images, alpha, train_step)\n \n    # Take the gradient of the scores with respect to the images\n    gradient = torch.autograd.grad(\n        inputs=interpolated_images,\n        outputs=mixed_scores,\n        grad_outputs=torch.ones_like(mixed_scores),\n        create_graph=True,\n        retain_graph=True,\n    )[0]\n    gradient = gradient.view(gradient.shape[0], -1)\n    gradient_norm = gradient.norm(2, dim=1)\n    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n    return gradient_penalty","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_examples(gen, steps, n=100):\n\n    gen.eval()\n    alpha = 1.0\n    for i in range(n):\n        with torch.no_grad():\n            noise = torch.randn(1, Z_DIM, 1, 1).to(DEVICE)\n            img = gen(noise, alpha, steps)\n            if not os.path.exists(f'saved_examples/step{steps}'):\n                os.makedirs(f'saved_examples/step{steps}')\n            save_image(img*0.5+0.5, f\"saved_examples/step{steps}/img_{i}.png\")\n    gen.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(\n    critic,\n    gen,\n    loader,\n    dataset,\n    step,\n    alpha,\n    opt_critic,\n    opt_gen,\n):\n    loop = tqdm(loader, leave=True)\n    for batch_idx, (real, _) in enumerate(loop):\n        real = real.to(DEVICE)\n        cur_batch_size = real.shape[0]\n\n        noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(DEVICE)\n\n        fake = gen(noise, alpha, step)\n        critic_real = critic(real, alpha, step)\n        critic_fake = critic(fake.detach(), alpha, step)\n        gp = gradient_penalty(critic, real, fake, alpha, step, device=DEVICE)\n        loss_critic = (\n            -(torch.mean(critic_real) - torch.mean(critic_fake))\n            + LAMBDA_GP * gp\n            + (0.001 * torch.mean(critic_real ** 2))\n        )\n\n        critic.zero_grad()\n        loss_critic.backward()\n        opt_critic.step()\n\n        gen_fake = critic(fake, alpha, step)\n        loss_gen = -torch.mean(gen_fake)\n\n        gen.zero_grad()\n        loss_gen.backward()\n        opt_gen.step()\n\n        alpha += cur_batch_size / (\n            (PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset)\n        )\n        alpha = min(alpha, 1)\n\n        loop.set_postfix(\n            gp=gp.item(),\n            loss_critic=loss_critic.item(),\n        )\n        \n\n    return alpha\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen = Generator(\n    Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG\n).to(DEVICE)\ncritic = Discriminator(\n    Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG\n).to(DEVICE)\n\nopt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\nopt_critic = optim.Adam(\n    critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99)\n)\nscaler_critic = torch.cuda.amp.GradScaler()\nscaler_gen = torch.cuda.amp.GradScaler()\n\n\n\n\ngen.train()\ncritic.train()\n\ntensorboard_step = 0\nstep = int(log2(START_TRAIN_AT_IMG_SIZE / 4))\nfor num_epochs in PROGRESSIVE_EPOCHS[step:]:\n    alpha = 1e-5  \n    loader, dataset = get_loader(4 * 2 ** step)  \n    print(f\"Current image size: {4 * 2 ** step}\")\n\n    for epoch in range(num_epochs):\n        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n        alpha = train_fn(\n            critic,\n            gen,\n            loader,\n            dataset,\n            step,\n            alpha,\n            opt_critic,\n            opt_gen,\n        )\n    generate_examples(gen, step, n=100)\n\n\n    step += 1  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}